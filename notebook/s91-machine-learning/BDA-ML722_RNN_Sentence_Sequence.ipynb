{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industry 4.0 의 중심, AI - ML&DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='right'><font size=2 color='gray'>Machine Learning & Deep Learning with TensorFlow @ <font color='blue'><a href='https://www.facebook.com/jskim.kr'>FB / jskim.kr</a></font>, 김진수</font></div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sect12. RNN (Recurrent Newural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from images import bigpycraft_ai as bpc\n",
    "from IPython.display import Image \n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "def chk_processting_time(start_time, end_time):\n",
    "    process_time = end_time - start_time\n",
    "    p_time = int(process_time)\n",
    "    p_min = p_time // 60\n",
    "    p_sec = p_time %  60\n",
    "    print('처리시간 : {p_min}분 {p_sec}초 경과되었습니다.'.format(\n",
    "            p_min = p_min, \n",
    "            p_sec = p_sec\n",
    "        ))\n",
    "    return process_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12-2. rnn_char_seq\n",
    "\n",
    "<hr>\n",
    "``` python \n",
    "\n",
    "# Lab 12 Character Sequence RNN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample))  # index -> char\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n",
    "\n",
    "# hyper parameters\n",
    "dic_size = len(char2idx)  # RNN input size (one hot size)\n",
    "rnn_hidden_size = len(char2idx)  # RNN output size\n",
    "num_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\n",
    "batch_size = 1  # one sample data, one batch\n",
    "sequence_length = len(sample) - 1  # number of lstm rollings (unit #)\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample]  # char to index\n",
    "x_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\n",
    "y_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])  # X data\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "x_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=rnn_hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(\n",
    "    cell, x_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t_cnt in range(3000):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "\n",
    "        print(t_cnt, \"loss:\", l, \"Prediction:\", ''.join(result_str))\n",
    "\n",
    "\n",
    "'''\n",
    " 1th training... \t Loss : 2.2902767658233643, \t Prediction : oouuouu u  u uu \n",
    " 2th training... \t Loss : 2.193434476852417, \t     Prediction : oo oou       o  \n",
    " 3th training... \t Loss : 2.090886354446411, \t     Prediction : y   ou          \n",
    " 4th training... \t Loss : 1.98934805393219, \t     Prediction : y   ou       ou \n",
    " 5th training... \t Loss : 1.8439476490020752, \t Prediction : y   ou      you \n",
    " 6th training... \t Loss : 1.7353318929672241, \t Prediction : y   ou      yo  \n",
    " 7th training... \t Loss : 1.6576930284500122, \t Prediction : yf you      you \n",
    " 8th training... \t Loss : 1.681345820426941, \t     Prediction : yf oou w nt yo  \n",
    " 9th training... \t Loss : 1.5772677659988403, \t Prediction : yf oou wwnt you \n",
    " 10th training... \t Loss : 1.5574511289596558, \t Prediction : yf you wwnt you\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    " 2995th training... \t Loss : 0.9896155595779419, \t Prediction : if you want you \n",
    " 2996th training... \t Loss : 0.9896155595779419, \t Prediction : if you want you \n",
    " 2997th training... \t Loss : 0.9896154403686523, \t Prediction : if you want you \n",
    " 2998th training... \t Loss : 0.9896154403686523, \t Prediction : if you want you \n",
    " 2999th training... \t Loss : 0.9896154403686523, \t Prediction : if you want you \n",
    " 3000th training... \t Loss : 0.9896153807640076, \t Prediction : if you want you \n",
    "'''\n",
    "\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda3-50\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Lab 12 Character Sequence RNN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 9,\n",
       " '구': 0,\n",
       " '그': 11,\n",
       " '누': 15,\n",
       " '당': 4,\n",
       " '랑': 14,\n",
       " '만': 10,\n",
       " '면': 13,\n",
       " '빠': 12,\n",
       " '사': 6,\n",
       " '신': 8,\n",
       " '약': 2,\n",
       " '에': 1,\n",
       " '와': 7,\n",
       " '이': 5,\n",
       " '지': 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample = \" if you want you\"\n",
    "sample   = \"만약에 당신이 그 누구와 사랑에 빠지면\"\n",
    "idx2char = list(set(sample))  # index -> char\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n",
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 16, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper parameters\n",
    "dic_size        = len(char2idx)     # RNN input size (one hot size)\n",
    "rnn_hidden_size = len(char2idx)     # RNN output size\n",
    "num_classes     = len(char2idx)     # final output size (RNN or softmax, etc.)\n",
    "batch_size      = 1                 # one sample data, one batch\n",
    "sequence_length = len(sample) - 1   # number of lstm rollings (unit #)\n",
    "\n",
    "dic_size, rnn_hidden_size, num_classes, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data : [[10, 2, 1, 9, 4, 8, 5, 9, 11, 9, 15, 0, 7, 9, 6, 14, 1, 9, 12, 3]]\n",
      "y_data : [[2, 1, 9, 4, 8, 5, 9, 11, 9, 15, 0, 7, 9, 6, 14, 1, 9, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "sample_idx = [char2idx[c] for c in sample]  # char to index\n",
    "x_data = [sample_idx[:-1]]    # X data sample (0 ~ n-1) hello: hell\n",
    "y_data = [sample_idx[1:]]     # Y label sample (1 ~ n) hello: ello\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])  # X data\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "print('x_data : {}'.format(x_data))\n",
    "print('y_data : {}'.format(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=rnn_hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(\n",
    "    cell, x_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1th training... \t Loss : 2.7593472003936768, \t Prediction :                면   면 \n",
      " 2th training... \t Loss : 2.58374285697937, \t Prediction :                      \n",
      " 3th training... \t Loss : 2.469827651977539, \t Prediction :                      \n",
      " 4th training... \t Loss : 2.3028340339660645, \t Prediction : 약      그             \n",
      " 5th training... \t Loss : 2.2103445529937744, \t Prediction : 약             랑    면 \n",
      " 6th training... \t Loss : 2.124105215072632, \t Prediction : 약            랑랑랑     \n",
      " 7th training... \t Loss : 2.068150281906128, \t Prediction : 약에 당   그     랑랑      \n",
      " 8th training... \t Loss : 2.0155274868011475, \t Prediction : 약에 당 이 그      랑      \n",
      " 9th training... \t Loss : 1.9337736368179321, \t Prediction : 약에 당   그             \n",
      " 10th training... \t Loss : 1.8768390417099, \t Prediction : 약에 당신                \n",
      " 11th training... \t Loss : 1.8428661823272705, \t Prediction : 약에 당신이               \n",
      " 12th training... \t Loss : 1.793089747428894, \t Prediction : 약에 당신이 그           면 \n",
      " 13th training... \t Loss : 1.7331058979034424, \t Prediction : 약에 당신이 그          지면 \n",
      " 14th training... \t Loss : 1.6982473134994507, \t Prediction : 약에 당신이 그         빠 면 \n",
      " 15th training... \t Loss : 1.675807237625122, \t Prediction : 약에 당이이 그 누누  사   빠지면 \n",
      " 16th training... \t Loss : 1.6353695392608643, \t Prediction : 약에 당이이 그 누누와 사   빠지면 \n",
      " 17th training... \t Loss : 1.609193205833435, \t Prediction : 약에 당신이 그 누누와 사   빠지면 \n",
      " 18th training... \t Loss : 1.5932811498641968, \t Prediction : 약에 당신이 그 누구와 사 에 빠지면 \n",
      " 19th training... \t Loss : 1.576229453086853, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 100th training... \t Loss : 1.3421331644058228, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 200th training... \t Loss : 1.3326374292373657, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 300th training... \t Loss : 1.329081416130066, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 400th training... \t Loss : 1.3283158540725708, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 500th training... \t Loss : 1.3226298093795776, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 600th training... \t Loss : 1.321346640586853, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 700th training... \t Loss : 1.3204433917999268, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 800th training... \t Loss : 1.3199461698532104, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 900th training... \t Loss : 1.3197104930877686, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 1000th training... \t Loss : 1.3195494413375854, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 1100th training... \t Loss : 1.3194270133972168, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 1200th training... \t Loss : 1.3193292617797852, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 1300th training... \t Loss : 1.3192510604858398, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 1400th training... \t Loss : 1.3191825151443481, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 1500th training... \t Loss : 1.3191251754760742, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 1600th training... \t Loss : 1.319077730178833, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 1700th training... \t Loss : 1.3190336227416992, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 1800th training... \t Loss : 1.318995714187622, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 1900th training... \t Loss : 1.3189620971679688, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 2000th training... \t Loss : 1.3189325332641602, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 2100th training... \t Loss : 1.3189053535461426, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 2200th training... \t Loss : 1.3188811540603638, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 2300th training... \t Loss : 1.3188618421554565, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 2400th training... \t Loss : 1.3188389539718628, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 2500th training... \t Loss : 1.3188204765319824, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 2600th training... \t Loss : 1.3188081979751587, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 2700th training... \t Loss : 1.3188068866729736, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 2800th training... \t Loss : 1.318773865699768, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 2900th training... \t Loss : 1.3187618255615234, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n",
      " 3000th training... \t Loss : 1.3187626600265503, \t Prediction : 약에 당신이 그 누구와 사랑에 빠지면 \n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t_cnt in range(3000):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        \n",
    "        cnt = t_cnt+1\n",
    "        if cnt<20 or cnt%100==0:\n",
    "            # print(t_cnt, \"loss:\", l, \"Prediction:\", ''.join(result_str))\n",
    "            print(\" {cnt}th training... \\t Loss : {loss}, \\t Prediction : {pred} \".format(\n",
    "                    cnt  = cnt, \n",
    "                    loss = l, \n",
    "                    pred = ''.join(result_str)\n",
    "            ))\n",
    "\n",
    "time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리시간 : 0분 15초 경과되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.541375160217285"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk_processting_time(time1, time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'만약에 당신이 그 누구와 사랑에 빠지면'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence = sample[0] + ''.join(result_str)\n",
    "predict_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "<marquee><font size=3 color='brown'>The BigpyCraft find the information to design valuable society with Technology & Craft.</font></marquee>\n",
    "<div align='right'><font size=2 color='gray'> &lt; The End &gt; </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
