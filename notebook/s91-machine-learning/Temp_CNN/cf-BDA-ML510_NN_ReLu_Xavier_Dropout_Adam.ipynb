{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<b><font size=6>Industry 4.0 의 중심, AI - ML&DL</font></b>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='right'><font size=2 color='gray'>Machine Learning & Deep Learning with TensorFlow @ <font color='blue'><a href='https://www.facebook.com/jskim.kr'>FB / jskim.kr</a></font>, 김진수</font></div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sect10. NN, ReLu, Xavier, Dropout, and Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from images import bigpycraft_copy as bpc\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!--\n",
    "### Examples : https://github.com/aymericdamien/TensorFlow-Examples\n",
    "//-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#00AAAA'> \n",
    "# MNIST Dataset Introduction\n",
    "<br>\n",
    "Most examples are using MNIST dataset of handwritten digits. It has 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image, so each sample is represented as a matrix of size 28x28 with values from 0 to 1.\n",
    "\n",
    "### Overview\n",
    "\n",
    "![MNIST Digits](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "### Usage\n",
    "In our examples, we are using TensorFlow [input_data.py](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/input_data.py) script to load that dataset.\n",
    "It is quite useful for managing our data, and handle:\n",
    "\n",
    "- Dataset downloading\n",
    "\n",
    "- Loading the entire dataset into numpy array: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Load data\n",
    "X_train = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "Y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='00AAAA'> \n",
    "- A \"next_batch\" function that can iterate over the whole dataset and return only the desired fraction of the dataset samples (in order to save memory and avoid to load the entire dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the next 64 images array and labels\n",
    "batch_X, batch_Y = mnist.train.next_batch(64)\n",
    "batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "time.sleep(3)\n",
    "time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리시간 : 0분 3초 경과되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0000243186950684"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bpc.chk_processting_time(time1, time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Softmax classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Review : Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders, 784(=28*28)\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Signature: tf.reduce_mean(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None)\n",
    "Docstring:\n",
    "Computes the mean of elements across dimensions of a tensor.\n",
    "\n",
    "Args:\n",
    "  input_tensor: The tensor to reduce. Should have numeric type.\n",
    "  axis: The dimensions to reduce. If `None` (the default), reduces all dimensions.\n",
    "  keep_dims: If true, retains reduced dimensions with length 1.\n",
    "  name: A name for the operation (optional).\n",
    "  reduction_indices: The old (deprecated) name for axis.\n",
    "\n",
    "Returns:\n",
    "  The reduced tensor."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Signature: tf.nn.softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None)\n",
    "Docstring:\n",
    "Computes softmax cross entropy between `logits` and `labels`.\n",
    "\n",
    "Args:\n",
    "  _sentinel: Used to prevent positional parameters. Internal, do not use.\n",
    "  labels: Each row `labels[i]` must be a valid probability distribution.\n",
    "  logits: Unscaled log probabilities.\n",
    "  dim: The class dimension. Defaulted to -1 which is the last dimension.\n",
    "  name: A name for the operation (optional).\n",
    "\n",
    "Returns:\n",
    "  A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the\n",
    "  softmax cross entropy loss."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Init signature: tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
    "Docstring:     \n",
    "Optimizer that implements the Adam algorithm.\n",
    "\n",
    "Args:\n",
    "  learning_rate: A Tensor or a floating point value.  The learning rate.\n",
    "  beta1: A float value or a constant float tensor.\n",
    "         The exponential decay rate for the 1st moment estimates.\n",
    "  beta2: A float value or a constant float tensor.\n",
    "         The exponential decay rate for the 2nd moment estimates.\n",
    "  epsilon: A small constant for numerical stability.\n",
    "  use_locking: If True use locks for update operations.\n",
    "  name: Optional name for the operations created when applying gradients.\n",
    "        Defaults to \"Adam\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001  | cost = 5.745170969\n",
      "Epoch: 0002  | cost = 1.780056698\n",
      "Epoch: 0003  | cost = 1.122778631\n",
      "Epoch: 0004  | cost = 0.872012247\n",
      "Epoch: 0005  | cost = 0.738203178\n",
      "Epoch: 0006  | cost = 0.654728883\n",
      "Epoch: 0007  | cost = 0.596023602\n",
      "Epoch: 0008  | cost = 0.552216816\n",
      "Epoch: 0009  | cost = 0.518254962\n",
      "Epoch: 0010  | cost = 0.491113193\n",
      "Epoch: 0011  | cost = 0.468347534\n",
      "Epoch: 0012  | cost = 0.449374341\n",
      "Epoch: 0013  | cost = 0.432675650\n",
      "Epoch: 0014  | cost = 0.418828148\n",
      "Epoch: 0015  | cost = 0.406128927\n",
      "Learning Finished!\n",
      "처리시간 : 0분 9초 경과되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.371027946472168"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "time1 = time.time()\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), ' | cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "time2 = time.time()\n",
    "\n",
    "# bpc.chk_processting_time(time1, time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training_epochs, mnist.train.num_examples, total_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9023\n",
      "Label:  [8]\n",
      "Prediction:  [8]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22fbd9768d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NN(Newral Nets) for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "# b1 = tf.Variable(tf.random_normal([256]))\n",
    "# L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "# b2 = tf.Variable(tf.random_normal([256]))\n",
    "# L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "# b3 = tf.Variable(tf.random_normal([10]))\n",
    "# hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# tf Graph input\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "# Softmax loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))     \n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001  | cost = 177.982685512\n",
      "Epoch: 0002  | cost = 38.858910462\n",
      "Epoch: 0003  | cost = 24.135219411\n",
      "Epoch: 0004  | cost = 16.895824688\n",
      "Epoch: 0005  | cost = 12.251669669\n",
      "Epoch: 0006  | cost = 9.134999403\n",
      "Epoch: 0007  | cost = 6.787242962\n",
      "Epoch: 0008  | cost = 5.088744562\n",
      "Epoch: 0009  | cost = 3.857519612\n",
      "Epoch: 0010  | cost = 2.877424658\n",
      "Epoch: 0011  | cost = 2.197693292\n",
      "Epoch: 0012  | cost = 1.596975659\n",
      "Epoch: 0013  | cost = 1.288613827\n",
      "Epoch: 0014  | cost = 0.947307379\n",
      "Epoch: 0015  | cost = 0.865229058\n",
      "Learning Finished!\n",
      "처리시간 : 0분 47초 경과되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47.31706404685974"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "time1 = time.time()\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), ' | cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "time2 = time.time()\n",
    "\n",
    "# bpc.chk_processting_time(time1, time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 55000, 550)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_epochs, mnist.train.num_examples, total_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9465\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22fb9658160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Xavier initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방법 : initializer=tf.contrib.layers.xavier_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001  | cost = 0.307517038\n",
      "Epoch: 0002  | cost = 0.118194564\n",
      "Epoch: 0003  | cost = 0.077469013\n",
      "Epoch: 0004  | cost = 0.054041942\n",
      "Epoch: 0005  | cost = 0.039921121\n",
      "Epoch: 0006  | cost = 0.030500028\n",
      "Epoch: 0007  | cost = 0.026196581\n",
      "Epoch: 0008  | cost = 0.020262506\n",
      "Epoch: 0009  | cost = 0.016947473\n",
      "Epoch: 0010  | cost = 0.013304636\n",
      "Epoch: 0011  | cost = 0.013447090\n",
      "Epoch: 0012  | cost = 0.010513373\n",
      "Epoch: 0013  | cost = 0.011956148\n",
      "Epoch: 0014  | cost = 0.009481524\n",
      "Epoch: 0015  | cost = 0.009878563\n",
      "Learning Finished!\n",
      "처리시간 : 0분 51초 경과되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.70966100692749"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "time1 = time.time()\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), ' | cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "time2 = time.time()\n",
    "\n",
    "# bpc.chk_processting_time(time1, time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9781\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22fbd5b2860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "<marquee><font size=3 color='brown'>The BigpyCraft find the information to design valuable society with Technology & Craft.</font></marquee>\n",
    "<div align='right'><font size=2 color='gray'> &lt; The End &gt; </font></div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
