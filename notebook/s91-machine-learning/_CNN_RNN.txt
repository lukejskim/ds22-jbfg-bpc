## Machine Learning for BigData

- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML101-Basics_operations.html                 "> 1-1. Basics operations                  </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML102-Linear_Regression.html                 "> 1-2. Linear Regression                  </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML103-Minimizing_Cost.html                   "> 1-3. Minimizing Cost                    </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML104-Multi-Variable_linear_regression.html  "> 1-4. Multi-Variable linear regression   </a>
<br/><br/>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML201-Logistic_classifier_ver2.html          "> 2-1. Logistic classifier                </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML202-Softmax_Classifier.html                "> 2-2. Softmax Classifier                 </a>
<br/><br/>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML301-Learning_rate_Evaluation.html          "> 3-1. Learning rate Evaluation           </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML302-Tensor_Manipulation.html               "> 3-2. Tensor Manipulation                </a>
<br/><br/>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML401-Neural_Networks.html                   "> 4-1. Neural Networks 1                  </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML402_Neural_Networks.html                   "> 4-2. Neural Networks 2                  </a>
    - <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML402_Neural_Networks_2-1.html           "> 4-2-1. Neural Networks (1)              </a>
    - <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML402_Neural_Networks_2-2.html           "> 4-2-2. Neural Networks (2)              </a>
<br/><br/>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML511-Softmax_for_MNIST.html                 "> 5-1. Softmax for MNIST                  </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML512-NN_for_MNIST.html                      "> 5-2. NN for MNIST                       </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML513-MNIST_NN_Xavier2.html                  "> 5-3. MNIST NN Xavier initialization(1)     </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML513-MNIST_NN_Xavier3.html                  "> 5-3. MNIST NN Xavier initialization(2)     </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML514-MNIST_NN_deep.html                     "> 5-4. MNIST NN deep                      </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML515-MNIST_NN_Dropout.html                  "> 5-5. MNIST NN Dropout                   </a>
<br/><br/>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML516-MNIST_NN_Batchnorm.html                "> Ref1. MNIST NN Batchnorm                 </a>
- <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-E/html/BDA-ML517-MNIST_NN_Higher_Level_API.html         "> Ref2. MNIST NN Higher Level API          </a>
<br/><br/>


..
BDA-ML101-Basics_operations.html
BDA-ML102-Linear_Regression.html
BDA-ML103-Minimizing_Cost.html
BDA-ML104-Multi-Variable_linear_regression.html
BDA-ML201-Logistic_classifier_ver2.html
BDA-ML202-Softmax_Classifier.html
BDA-ML301-Learning_rate_Evaluation.html
BDA-ML302-Tensor_Manipulation.html
BDA-ML401-Neural_Networks.html
BDA-ML402_Neural_Networks.html
BDA-ML510_NN_ReLu_Xavier_Dropout_Adam.html

BDA-ML510_NN_ReLu_Xavier_Dropout_Adam.html
BDA-ML511-Softmax_for_MNIST.html
BDA-ML512-NN_for_MNIST.html
BDA-ML513-MNIST_NN_Xavier2.html
BDA-ML514-MNIST_NN_deep.html
BDA-ML515-MNIST_NN_Dropout.html
BDA-ML516-MNIST_NN_Batchnorm.html
BDA-ML517-MNIST_NN_Higher_Level_API.html


5-1. Softmax for MNIST          
5-2. NN for MNIST               
5-3. MNIST NN Xavier2           
5-4. MNIST NN deep              
5-5. MNIST NN Dropout           
5-6. MNIST NN Batchnorm         
5-7. MNIST NN Higher Level API  


############################################
git status
git add .
git commit -m "Bigpy Create : Machine Learning for BigData"
git push origin +master
############################################

1-1. Basics operations                 
1-2. Linear Regression                 
1-3. Minimizing Cost                   
1-4. Multi-Variable linear regression  

BDA-ML101-Basics_operations.html                
BDA-ML102-Linear_Regression.html                
BDA-ML103-Minimizing_Cost.html                  
BDA-ML104-Multi-Variable_linear_regression.html 




<hr>

### TextBook : Web Crawling & Scraping

<table align="left">
    <tr align="left">
        <td width="300">
            <a href="https://www.seleniumhq.org/projects/webdriver/">
            <img src="../images/reference-03.png" width="250" />
            </a>
        </td>
        <td width="700">
<div align="left">
<font size="4">
01. <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-D/html/WCS01_Wget_크롤링.html              "> 크롤링과 스크레이핑이란?                  </a>
<br/><br/>
02. <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-D/html/WCS02_크롤링과_스크랩핑_ver2.html   "> 파이썬으로 시작하는 크롤링/스크레이핑     </a>
<br/><br/>
03. <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-D/html/WCS03_주요_라이브러리_활용_ver2.html"> 주요 라이브러리 활용                      </a>
<br/><br/>
04. <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-D/html/WCS04_크롤러사용_ver2.html          "> 크롤러를 사용할 때 기억해야 하는 것       </a>
<br/><br/>
05. <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-D/html/WCS05_데이터활용_ver3.html          "> 크롤링/스크레이핑 실전과 데이터 활용      </a>
<br/><br/>
06. <a href="https://htmlpreview.github.io/?https://github.com/bigpycraft/iitp18-multicampus/blob/master/section-D/html/WCS06_Scrapy_프레임워크_ver2.html   "> Scrapy 프레임워크      </a>
<br/><br/>
</font>
</div></td>
    </tr>
</table>
<br/>

WCS05_데이터활용_ver3



## <font color='brown'>파이썬을 이용한 웹크롤링과 스크레이핑 </font>
> 데이터 수집과 분석을 위한 실전 가이드
<br/><br/>
INDEX.
<br/>
<select name="index">
    <option value="CH00"> 파이썬을 이용한 웹크롤링과 스크레이핑, 데이터 수집과 분석을 위한 실전 가이드 </option>
    <option value="CH00"> ------------------------------------------------------------------------------------------------------------------ </option>
    <option value="CH01" selected> 01.크롤링과 스크레이핑이란?                      </option>
    <option value="CH02"> 02.파이썬으로 시작하는 크롤링/스크레이핑         </option>
    <option value="CH03"> 03.주요 라이브러리 활용                          </option>
    <option value="CH04"> 04.크롤러를 사용할 때 기억해야 하는 것           </option>
    <option value="CH05"> 05.크롤링/스크레이핑 실전과 데이터 활용          </option>
    <option value="CH06"> 06.Scrapy 프레임워크                             </option>
    <option value="CH07"> 07.크롤러의 지속적 운용과 관리                   </option>
</select>
<br/><br/>
APPENDIX
<br/>
<select name="appendix">
    <option value="A00"> 부록: 베이그런트로 개발 환경 구축하기 </option>
    <option value="A00"> ------------------------------------------------------------------------------------------------------------------ </option>
    <option value="A01"> A.1 버추얼박스와 베이그런트             </option>
    <option value="A02"> A.2 CPU 가상화 지원 기능 활성화하기     </option>
    <option value="A03"> A.3 버추얼박스 설치하기                 </option>
    <option value="A04"> A.4 베이그런트 설치하기                 </option>
    <option value="A05"> A.5 가상 머신 실행하기                  </option>
    <option value="A06"> A.6 게스트 OS에 SSH 접속하기            </option>
    <option value="A07"> A.7 리눅스 기본 조작                    </option>
    <option value="A07"> A.8 베이그런트의 가상 머신 조작 명령어  </option>
</select>


###############################################################
git status
git add .
git commit -m "Bigpy Update : Web Crawling & Scraping"
git push origin +master
###############################################################

<!--
### Index
> 데이터 수집과 분석을 위한 실전 가이드
1. 크롤링과 스크레이핑이란?
2. 파이썬으로 시작하는 크롤링/스크레이핑
3. 주요 라이브러리 활용
4. 크롤러를 사용할 때 기억해야 하는 것
5. 크롤링/스크레이핑 실전과 데이터 활용
6. Scrapy 프레임워크
7. 크롤러의 지속적 운용과 관리

### Appendix
> 부록: 베이그런트로 개발 환경 구축하기
A.1 버추얼박스와 베이그런트
A.2 CPU 가상화 지원 기능 활성화하기
A.3 버추얼박스 설치하기
A.4 베이그런트 설치하기
A.5 가상 머신 실행하기
A.6 게스트 OS에 SSH 접속하기
A.7 리눅스 기본 조작
A.8 베이그런트의 가상 머신 조작 명령어

//-->

- 라이브러리 설치
- 웹 페이지 간단하게 추출
- HTML 스크레이핑
- RSS 스크레이핑하기
- 데이터베이스에 저장
- 크롤러와 URL
- 파이썬으로 크롤러 만들기

<hr>
### <font color='#0000CC'>  라이브러리 설치
<hr>
### <font color='#0000CC'>  웹 페이지 간단하게 추출
<hr>
### <font color='#0000CC'>  HTML 스크레이핑
<hr>
### <font color='#0000CC'>  RSS 스크레이핑하기
<hr>
### <font color='#0000CC'>  데이터베이스에 저장
<hr>
### <font color='#0000CC'>  크롤러와 URL
<hr>
### <font color='#0000CC'>  파이썬으로 크롤러 만들기




=====================================================================================================================================
01장: 크롤링과 스크레이핑이란?
1.1 이 책에서 다루는 영역
1.2 Wget으로 크롤링하기
1.3 유닉스 명령어로 스크레이핑하기
1.4 한빛미디어의 전체 도서 목록 중에서 페이지 하나 출력하기
1.5 정리

02장: 파이썬으로 시작하는 크롤링/스크레이핑
2.1 파이썬을 사용할 때의 장점
2.2 파이썬 설치와 실행
2.3 파이썬 기초 지식
2.4 웹 페이지 추출하기
2.5 웹 페이지에서 데이터 추출하기
2.6 데이터 저장하기
2.7 파이썬으로 스크레이핑하는 흐름
2.8 정리

03장: 주요 라이브러리 활용
3.1 라이브러리 설치
3.2 웹 페이지 간단하게 추출하기
3.3 HTML 스크레이핑
3.4 RSS 스크레이핑하기
3.5 데이터베이스에 저장하기
3.6 크롤러와 URL
3.7 파이썬으로 크롤러 만들기
3.8 정리

04장: 크롤러를 사용할 때 기억해야 하는 것
4.1 크롤러 분류하기
4.2 크롤러를 만들 때 주의해야 하는 것
4.3 여러 번 사용을 전제로 설계하기
4.4 크롤링 대상의 변화에 대응하기
4.5 정리

05장: 크롤링/스크레이핑 실전과 데이터 활용
5.1 데이터 세트 추출과 활용
5.2 API로 데이터 수집하고 활용하기
5.3 시계열 데이터 수집하고 활용하기
5.4 열린 데이터 수집과 활용
5.5 웹 페이지 자동 조작
5.6 자바스크립트를 이용한 페이지 스크레이핑
5.7 추출한 데이터 활용하기
5.8 정리

06장: Scrapy 프레임워크
6.1 Scrapy 개요
6.2 Spider 만들고 실행하기
6.3 실전적인 크롤링
6.4 추출한 데이터 처리하기
6.5 Scrapy 설정
6.6 Scrapy 확장하기
6.7 크롤링으로 데이터 수집하고 활용하기
6.8 이미지 수집과 활용
6.9 정리

07장: 크롤러의 지속적 운용과 관리
7.1 크롤러를 서버에서 실행하기
7.2 크롤러를 정기적으로 실행하기
7.3 크롤링과 스크레이핑 분리하기
7.4 크롤링 성능 향상과 비동기 처리
7.5 클라우드 활용하기
7.6 정리

부록: 베이그런트로 개발 환경 구축하기
A.1 버추얼박스와 베이그런트
A.2 CPU 가상화 지원 기능 활성화하기
A.3 버추얼박스 설치하기
A.4 베이그런트 설치하기
A.5 가상 머신 실행하기
A.6 게스트 OS에 SSH 접속하기
A.7 리눅스 기본 조작
A.8 베이그런트의 가상 머신 조작 명령어


=====================================================================================================================================

▣ 01장: 크롤링과 스크레이핑이란?
1.1 이 책에서 다루는 영역
___1.1.1 크롤링과 스크레이핑
___1.1.2 크롤링/스크레이핑과 파이썬
___1.1.3 이 책에서 사용하는 플랫폼
___1.1.4 이 책의 구성
1.2 Wget으로 크롤링하기
___1.2.1 Wget이란?
___1.2.2 wget 사용법
___1.2.3 실제 사이트 크롤링하기
1.3 유닉스 명령어로 스크레이핑하기
___1.3.1 유닉스 명령어 기초 지식
___1.3.2 텍스트 처리와 관련된 유닉스 명령어
___1.3.3 정규 표현식
1.4 한빛미디어의 전체 도서 목록 중에서 페이지 하나 출력하기
___1.4.1 도서 목록 추출하기
1.5 정리

▣ 02장: 파이썬으로 시작하는 크롤링/스크레이핑
2.1 파이썬을 사용할 때의 장점
___2.1.1 언어 자체의 특
___2.1.2 강력한 서드파티 라이브러리
___2.1.3 스크레이핑 이후 처리와의 친화성
2.2 파이썬 설치와 실행
___2.2.1 파이썬 2와 파이썬 3
___2.2.2 패키지 매니저로 파이썬 3 설치하기
___2.2.3 가상 환경(venv) 사용하기
___2.2.4 인터랙티브 셸 사용
2.3 파이썬 기초 지식
___2.3.1 스크립트 파일 실행과 구성
___2.3.2 기본적인 데이터 구조
___2.3.3 제어 구조와 함수/클래스 정의
___2.3.4 내장 함수
___2.3.5 모듈
2.4 웹 페이지 추출하기
___2.4.1 urllib으로 웹 페이지 추출하기
___2.4.2 문자 코드 다루기
2.5 웹 페이지에서 데이터 추출하기
___2.5.1 정규 표현식으로 스크레이핑하기
___2.5.2 XML(RSS) 스크레이핑
2.6 데이터 저장하기
___2.6.1 CSV 형식으로 저장하기
___2.6.2 JSON 형식으로 저장하기
___2.6.3 데이터베이스(SQLite3)에 저장하기
2.7 파이썬으로 스크레이핑하는 흐름
2.8 정리

▣ 03장: 주요 라이브러리 활용
3.1 라이브러리 설치
___3.1.1 pip으로 설치하기
3.2 웹 페이지 간단하게 추출하기
3.3 HTML 스크레이핑
___3.3.1 XPath와 CSS 선택자
___3.3.2 lxml로 스크레이핑하기
___3.3.3 Beautiful Soup로 스크레이핑하기
3.4 RSS 스크레이핑하기
3.5 데이터베이스에 저장하기
___3.5.1 MySQL에 데이터 저장하기
___3.5.2 MongoDB에 데이터 저장하기
3.6 크롤러와 URL
___3.6.1 URL 기초 기식
___3.6.2 퍼머링크와 링크 구조 패턴
___3.6.3 재실행을 고려한 데이터 설계
3.7 파이썬으로 크롤러 만들기
___3.7.1 목록 페이지에서 퍼머 링크 목록 추출하기
___3.7.2 상세 페이지에서 스크레이핑하기
___3.7.3 상세 페이지 크롤링하기
___3.7.4 스크레이핑한 데이터 저장하기
3.8 정리

▣ 04장: 크롤러를 사용할 때 기억해야 하는 것
4.1 크롤러 분류하기
___4.1.1 상태를 가지는 지로 분류하기
___4.1.2 자바스크립트를 실행할 수 있는지로 분류하기
___4.1.3 불특정 다수의 사이트를 대상하고 있는지로 분류하기
4.2 크롤러를 만들 때 주의해야 하는 것
___4.2.2 robots.txt로 크롤러에게 지시하기
___4.2.3 XML 사이트맵
___4.2.4 크롤링 대상에 대한 부하
___4.2.5 연락처 명시하기
___4.2.6 상태 코드와 오류 처리
4.3 여러 번 사용을 전제로 설계하기
___4.3.1 변경된 데이터만 추출하기
4.4 크롤링 대상의 변화에 대응하기
___4.4.1 변화 감지하기
___4.4.2 변화 통지하기
4.5 정리

▣ 05장: 크롤링/스크레이핑 실전과 데이터 활용
5.1 데이터 세트 추출과 활용
___5.1.1 위키백과 데이터 세트 다운로드하기
___5.1.2 자연어 처리를 사용한 빈출 단어 추출
5.2 API로 데이터 수집하고 활용하기
___5.2.1 트위터에서 데이터 수집하기
___5.2.2 유튜브에서 동영상 정보 수집하기
5.3 시계열 데이터 수집하고 활용하기
___5.3.1 환율 데이터 수집
___5.3.2 pandas와 CSV 파일
___5.3.3 그래프로 시각화하기
5.4 열린 데이터 수집과 활용
___5.4.1 열린 데이터란?
___5.4.2 PDF에서 데이터 추출하기
___5.4.3 Linked Open Data를 기반으로 데이터 수집하기
5.5 웹 페이지 자동 조작
___5.5.1 자동 조작 구현 방법
___5.5.2 네이버페이 주문 이력 추출하기
5.6 자바스크립트를 이용한 페이지 스크레이핑
___5.6.1 자바스크립트를 사용한 페이지에 대한 대응 방법
___5.6.2 PhantomJS 활용하기
___5.6.3 RSS 피드 생성하기
5.7 추출한 데이터 활용하기
___5.7.1 지도로 시각화하기
___5.7.2 BigQuery로 해석하기
5.8 정리

▣ 06장: Scrapy 프레임워크
6.1 Scrapy 개요
___6.1.1 Scrapy 설치
___6.1.2 Spider 실행하기
6.2 Spider 만들고 실행하기
___6.2.1 Scrapy 프로젝트 만들기
___6.2.2 Item 만들기
___6.2.3 Spider 만들기
___6.2.4 Scrapy Shell로 인터랙티브하게 스크레이핑하기
___6.2.5 Spider 실행하기
6.3 실전적인 크롤링
___6.3.1 크롤링으로 링크 순회하기
___6.3.2 XML 사이트맵을 사용해 크롤링하기
6.4 추출한 데이터 처리하기
___6.4.1 Item Pipeline 개요
___6.4.2 데이터 검증
___6.4.3 MongoDB에 데이터 저장하기
___6.4.4 MySQL에 데이터 저장하기
6.5 Scrapy 설정
___6.5.1 설정 방법
___6.5.2 크롤링 대상에 폐를 끼치지 않기 위한 설정 항목
___6.5.3 병렬 처리와 관련된 설정 항목
___6.5.4 HTTP 요청과 관련된 설정
___6.5.5 HTTP 캐시 설정 항목
___6.5.6 오류 처리와 관련된 설정
___6.5.7 프락시 사용하기
6.6 Scrapy 확장하기
___6.6.1 다운로드 처리 확장하기
___6.6.2 Spider의 동작 확장하기
6.7 크롤링으로 데이터 수집하고 활용하기
___6.7.1 음식점 정보 수집
___6.7.2 불특정 다수의 웹사이트 크롤링하기
6.8 이미지 수집과 활용
___6.8.1 플리커에서 이미지 수집하기
___6.8.2 OpenCV로 얼굴 이미지 추출하기
6.9 정리

▣ 07장: 크롤러의 지속적 운용과 관리
7.1 크롤러를 서버에서 실행하기
___7.1.1 가상 서버 만들기
___7.1.2 서버에 디플로이하기
7.2 크롤러를 정기적으로 실행하기
___7.2.1 Cron 설정
___7.2.2 오류 통지
7.3 크롤링과 스크레이핑 분리하기
___7.3.1 메시지 큐 RQ 사용 방법
___7.3.2 메시지 큐로 연동하기
___7.3.3 메시지 큐 운용하기
7.4 크롤링 성능 향상과 비동기 처리
___7.4.1 멀티 스레드와 멀티 프로세스
___7.4.2 비동기 I/O를 사용해 효율적으로 크롤링하기
7.5 클라우드 활용하기
___7.5.1 클라우드의 장점
___7.5.2 AWS SDK 사용하기
___7.5.3 클라우드 스토리지 사용하기
7.6 정리

▣ 부록: 베이그런트로 개발 환경 구축하기
A.1 버추얼박스와 베이그런트
___A.1.1 버추얼박스란?
___A.1.2 베이그런트란?
A.2 CPU 가상화 지원 기능 활성화하기
___A.2.1 윈도우 10의 경우
___A.2.2 윈도우 7의 경우
___A.2.3 펌웨어 설정으로 가상화 지원 기능 활성화하기
A.3 버추얼박스 설치하기
A.4 베이그런트 설치하기
A.5 가상 머신 실행하기
A.6 게스트 OS에 SSH 접속하기
___A.6.1 Tera Term 설치
___A.6.2 Tera Term으로 게스트 OS에 SSH로 접속하기
A.7 리눅스 기본 조작
___A.7.1 소프트웨어 설치하기
A.8 베이그런트의 가상 머신 조작 명령어
___A.8.1 가상 머신 실행하기(vagrant up)
___A.8.2 가상 머신 종료/재실행하기(vagrant halt/reload)
___A.8.3 가상 머신 제거하기(vagrant destroy)
___A.8.4 가상 머신 상태 출력하기(vagrant status)
___A.8.5 가상 머신에 SSH로 접속하기(vagrant ssh)
___A.8.6 가상 머신 익스포트하기(vagrant package)  닫기


